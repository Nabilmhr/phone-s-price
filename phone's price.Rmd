---
title: "Projet Data mining apprentissage"
output: html_document
date: "2024-03-26"
---

Le prix des t√©l√©phones portables d√©pend de plusieurs facteurs comme la marque, la taille, le poids, la qualit√© de l'image, la RAM, la batterie ou encore le processeur.

Product_id : ID of each cellphone.
Price : Price of each cellphone.
Sale : Sales number.
weight : Weight of each cellphone.
resoloution :  Resoloution of each cellphone.
ppi : Phone Pixel Density.
cpu core : Type of CPU core in each cellphone.
cpu freq : CPU Frequency in each cellphone.
internal mem : Internal memory of each cellphone.
ram : RAM of each cellphone.

Le but de ce projet est de pr√©dire/estimer le prix d'un t√©l√©phone √† partir de ses caract√©ristiques.


#Chargement des donn√©es
```{r setup, include=FALSE}
getwd()
donnee <- read.csv("Cellphone.csv")
```

#Chargement des packages
```{r}
library(knitr) 
library(tidyverse)
library(corrplot) 
library(scales) 
library(rpart) 
library(glmnet) 
library("ISLR") 
library(tidyverse) 
library(caret)
library(leaps) 
library(car) 
library(randomForest)
```

#Visualisation des donn√©es
```{r}
dim(donnee)

head(donnee)

str(donnee)

```
Les base de donn√©es est compos√©e de 161 observations et de 14 variables.
Toutes les variables sont quantitatives.


```{r}
summary(donnee)
```
Le prix moyen d'un t√©l√©phone est de 2216 ‚Ç¨ ?
On constate aucunes donn√©es manquantes dans notre base de donn√©es.

#Corr√©lation
```{r}
correlation <- cor(donnee)
corrplot(correlation, method = 'number',type = 'lower')
```
Les variables les plus corr√©l√©es positivement sont Resolution et Weight (la r√©solution et le poids du t√©l√©phone) avec un coefficient de corr√©lation de 0,89.

Les variables les plus corr√©l√©es n√©gativement sont Thickness et cpu.core (l'√©paisseur et le type de processeur du t√©l√©phone) avec un coefficient de corr√©lation de -0,70.

Dans un mod√®le de r√©gression, deux variables tr√®s corr√©l√©es entre elles peuvent fausser l‚Äôestimation des coefficients. Il est donc utile de tenir compte des relations ci-dessus.

La variable la plus corr√©l√©e positivement √† Price est ram (m√©moire n√©cessaire au bon fonctionnement du processeur) avec un coefficient de corr√©lation de 0,90. Donc la capacit√© de la RAM est un √©l√©ment important dans dans l'estimation du prix d'un t√©l√©phone.

La variable la plus corr√©l√©e n√©gativement √† Price est Thickness (l'√©paisseur du t√©l√©phone) avec un coefficient de corr√©lation de -0,72.
Donc plus le t√©l√©phone est √©pais, moins il est cher et inversement.


#Cr√©ation d'un √©chantillon d'apprentissage et de test

Le but de partitionner les donn√©es en base d'apprentissage et de test est de pouvoir √©valuer les performances de nos mod√®les d'apprentissage automatique.

L'√©valuation des performances sur un ensemble de test distinct est importante pour √©viter le sur-ajustement, cas dans lequel le mod√®le apprend les caract√©ristiques sp√©cifiques de l'ensemble de donn√©es d'apprentissage sans pouvoir g√©n√©raliser sur de nouvelles donn√©es.

Nous avons cr√©√© un ensemble d'apprentissage avec 75% des donn√©es (donneetrain) et un ensemble de tests avec les 25% restants (donneetest).

Nous utiliserons principalement le risque quadratique moyen (MSE) et la racine du risque quadratique moyen (RMSE) sur l‚Äô√©chantillon test, comme mesure de l‚Äôerreur de pr√©diction.

```{r}
set.seed(100)
indxTrain = createDataPartition(donnee$Price,p=0.75,list=FALSE)
donneetrain= donnee[indxTrain,]
donneetest = donnee[-indxTrain,]
```


#Mod√®le de r√©gression lin√©aire complet
```{r}
reg = lm(Price~., data=donneetrain)
summary(reg)
```
La variable d√©pendante est Price.
Les variables ind√©pendantes sont les 13 autres variables de notre base de donn√©es.

Le R^2 ajust√© est de 94,93 % ce qui correspond effectivement a un bon ajustement.

Cependant, le graphique ci-dessous montre que le mod√®le peut √™tre am√©lior√©.
```{r}
res=residuals(reg)
plot(reg,which=1:2)
```
√Ä noter que ce mod√®le nous servira de premi√®re r√©f√©rence pour √©valuer la performance des prochains mod√®les.

#Mod√®le AIC
```{r}
regAIC= step(reg, trace=TRUE)
extractAIC(regAIC)
```

Le mod√®le avec le plus petit AIC est consid√©r√© comme le meilleur mod√®le parmi les mod√®les candidats. Cela signifie que ce mod√®le repr√©sente un bon compromis entre la qualit√© de l'ajustement et la complexit√© du mod√®le.

Dans notres cas, le plus petit AIC est de 1269,49 et il correspond au mod√®le suivant (dernier mod√®le).


```{r}
modAIC = lm(Price ~ weight + ppi + cpu.core + cpu.freq + internal.mem + ram + 
    battery + thickness, data=donneetrain)

summary(modAIC)
extractAIC(modAIC)

```

Le mod√®le AIC a un R^2 ajust√© de 94,99%. Ce mod√®le est donc l√©g√®rement mieux ajust√© que le mod√®le complet.


#Mod√®le Cp de Mallows
```{r}
#Extraction des variables explicatives
donneetrain.x = donneetrain[,-2]

#Recherche des meilleurs mod√®les au sens du Cp
p.choix = leaps(donneetrain.x,donneetrain[,"Price"],method="Cp",nbest=1)

#R√©sultats
p.choix$Cp
plot(p.choix$size-1,p.choix$Cp)

#Meilleur mod√®le
t = (p.choix$Cp==min(p.choix$Cp))

#Liste des variables explicatives du meilleur mod√®le
colnames(donneetrain)[p.choix$which[t]]
```

```{r}
#Mod√®le s√©lectionn√© selon le Cp de Mallows
modCp = lm(Price ~ Sale + resoloution + ppi + cpu.core + cpu.freq + internal.mem + Front_Cam + battery, data=donneetrain)

summary(modCp)
```
Le mod√®le obtenu au sens du Cp de Mallows explique 94,05% de la variabilit√© du prix des t√©l√©phones.



#Mod√®le au sens du R2 ajust√©
```{r}
p.choix2 = leaps(donneetrain.x,donneetrain[,"Price"],method="adjr2",nbest=1)

plot(p.choix2$size-1,p.choix2$adjr2)

#Meilleur mod√®le
t = (p.choix2$adjr2==max(p.choix2$adjr2))

#Liste des variables explicatives du meilleur mod√®le
colnames(donneetrain)[p.choix2$which[t]]

```

```{r}
#Mod√®le s√©lectionn√© selon le R2 ajust√©
modAdjR2 = lm(Price ~ Sale + resoloution + ppi + cpu.core + cpu.freq + internal.mem + RearCam + Front_Cam + battery, data=donneetrain)

summary(modAdjR2)
```
Tout comme le AIC et le Cp de Mallow‚Äôs, le ùëÖ2 ajust√© est un crit√®re permettant d‚Äô√©valuer la qualit√© d‚Äôajustement d‚Äôun mod√®le. Tout en tenant compte du nombre de pr√©dicteurs, il mesure la proportion de variance expliqu√©e par les variables ind√©pendantes, dans la variance totale de la variable d√©pendante.

En consid√©rant ce crit√®re, le ùëÖ2 ajust√© du mod√®le s√©lectionn√© est de 93,83% et son risque quadratique moyen est de 441364613.

En termes d‚Äôajustement, le mod√®le est tr√®s similaire aux mod√®les pr√©c√©dents. En revanche, il est le moins performant de tous les mod√®les en termes de pr√©diction.

#Erreurs empiriques
```{r}
#mod√®le complet
pred = predict(reg, newdata= donneetest) 
err= mean((pred-donneetest$Price)^2)

#mod√®le AIC
predAIC = predict(modAIC, newdata= donneetest) 
errAIC= mean((predAIC-donneetest$Price)^2)

#mod√®le Cp Mallows
predCP = predict(modCp, newdata= donneetest) 
errCP= mean((predCP-donneetest$Price)^2)

#mod√®le R2 ajust√©
predAdjR2 = predict(modAdjR2, newdata= donneetest) 
errAdjr2= mean((predAdjR2-donneetest$Price)^2)
```

Si on regarde les erreurs de pr√©dictions sur l'√©chantillon test pour chaque mod√®le, on constate que le mod√®le complet a l'erreur la plus faible de 36302.

Au vu des r√©sultats pr√©c√©dents, le mod√®le AIC est celui avec le meilleur R2 ajust√©.